{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304b0c06",
   "metadata": {},
   "source": [
    "# Machine Learning Fundamentals - Lecture 03\n",
    "\n",
    "This is the Jupyter notebook for Lecture 03 of the Machine Learning Fundamentals\n",
    "course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f40c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries using the commonly use short names (pd, sns, ...)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# The Path object from pathlib allows us to easily build paths in an\n",
    "# OS-independent fashion\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the required scikit-learn classes and functions\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# Set a nicer style for Seaborn plots\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4f293",
   "metadata": {},
   "source": [
    "## Part 1: load and clean the Pokémon dataset\n",
    "\n",
    "Here we just repeat the steps already done in the previous lectures, but in a\n",
    "more succint way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "761fb22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (note the use of the Path object)\n",
    "df = pd.read_csv(Path(\"..\", \"datasets\", \"Pokemon.csv\"))\n",
    "\n",
    "# It's not good practice to have column names with spaces and other non-standard\n",
    "# characters, so let's fix this by renaming the columns to standard names\n",
    "df.rename(columns={\n",
    "    \"Type 1\" : \"Type1\",\n",
    "    \"Type 2\" : \"Type2\",\n",
    "    \"Sp. Atk\" : \"SpAtk\",\n",
    "    \"Sp. Def\" : \"SpDef\",\n",
    "}, inplace=True)\n",
    "\n",
    "# Replace missing values in the \"Type2\" column with the string \"None\"\n",
    "df[\"Type2\"] = df[\"Type2\"].fillna(\"None\")\n",
    "\n",
    "# Since primary and secondary types are essentially categories (and not just\n",
    "# strings / objects), we can convert these columns to the category type\n",
    "df[\"Type1\"] = df[\"Type1\"].astype(\"category\")\n",
    "df[\"Type2\"] = df[\"Type2\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07466c93",
   "metadata": {},
   "source": [
    "Before we proceed to the interesting part, we'll perform our data scaling and\n",
    "train/test data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41193ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use all features except the Total, which can be considered redundant\n",
    "# since it's the total of the other features\n",
    "features = [\"HP\", \"Attack\", \"Defense\", \"SpAtk\", \"SpDef\", \"Speed\"]\n",
    "\n",
    "# Get only the specified features\n",
    "df_X = df[features]\n",
    "\n",
    "# Standardize them\n",
    "ss = StandardScaler()\n",
    "X = ss.fit_transform(df_X)\n",
    "\n",
    "# Our labels will be the legendary status\n",
    "y_leg = df[\"Legendary\"].to_numpy()\n",
    "\n",
    "# Let's split our data into training (80%) and test (20%) sets\n",
    "# Change the random_state parameter do split data in different ways\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_leg, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648147a5",
   "metadata": {},
   "source": [
    "## Part 2: Implement our own $k$-Nearest Neighbors classifier and regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6ed975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this variable to change k for all the tests in this section\n",
    "k_for_all = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc5deb",
   "metadata": {},
   "source": [
    "### 2.1. A $k$-Nearest Neighbors classifier\n",
    "\n",
    "Let's start with the classifier. We'll use our implementation to classify\n",
    "legendary and non-legendary Pokémons, and compare our results with the actual\n",
    "$k$-NN classifier provided with `scikit-learn`.\n",
    "\n",
    "Our approach will use NumPy vectorization firstly for training your NumPy\n",
    "skills, and secondly to avoid for loops and increase performance.\n",
    "\n",
    "**Note**:\n",
    "\n",
    "The \n",
    "[`np.partition()`](https://numpy.org/doc/stable/reference/generated/numpy.partition.html)\n",
    "function rearranges an array so that the element at a chosen position ($k$) is\n",
    "in the place it would be if the array was sorted. All smaller elements than that\n",
    "$k$-th element go to the left, and all larger elements go to the right. The left\n",
    "and right parts are not fully sorted, just partitioned.\n",
    "\n",
    "The\n",
    "[`np.argpartition()`](https://numpy.org/doc/stable/reference/generated/numpy.argpartition.html)\n",
    "function does almost the same, but instead of returning the values, it returns\n",
    "the indices of how to rearrange the array to achieve that partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e84192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our classifier will be a simple function that does the training (fit) and\n",
    "# classification (predict) in one go\n",
    "def knn_classify(X_train, y_train, X_test, k=5):\n",
    "\n",
    "    # First, we'll calculate the Euclidean distance between the test examples\n",
    "    # and the training examples\n",
    "    dists = euclidean_distances(X_test, X_train)\n",
    "\n",
    "    # Now let's get the indices of the k neighbors closest to each test example\n",
    "    # See above the explanation of np.argpartition()\n",
    "    idx_k_min = np.argpartition(dists, k, axis=1)[:, :k]\n",
    "\n",
    "    # For each index, get its corresponding label from the training data\n",
    "    labels_k_min = y_train[idx_k_min]\n",
    "\n",
    "    # For each test example, get the majority label\n",
    "\n",
    "    # Instead of adding majority labels to a list as we go, let's pre-allocate a\n",
    "    # numpy array for this purpose, which is much more efficient\n",
    "    maj_labels = np.zeros(labels_k_min.shape[0], dtype=y_train.dtype)\n",
    "\n",
    "    # Loop through each row of the labels matrix and determine the majority label\n",
    "    for i, row in enumerate(labels_k_min):\n",
    "\n",
    "        # Get the unique labels in the current row as well as its count\n",
    "        values, counts = np.unique(row, return_counts=True)\n",
    "\n",
    "        # Get the label with highest count and put it in our pre-allocated\n",
    "        # numpy array\n",
    "        maj_labels[i] = values[np.argmax(counts)]\n",
    "\n",
    "    # Return our predictions\n",
    "    return maj_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b52a01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our kNN classifier: 0.925\n"
     ]
    }
   ],
   "source": [
    "# Let's classify some test data with our classifier\n",
    "y_pred_ours = knn_classify(X_train, y_train, X_test, k=k_for_all)\n",
    "\n",
    "# And check the accuracy of our prediction\n",
    "acc_ours = accuracy_score(y_test, y_pred_ours)\n",
    "\n",
    "# Print it\n",
    "print(f\"Accuracy of our kNN classifier: {acc_ours}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d820b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of scikit-learn's kNN classifier: 0.925\n"
     ]
    }
   ],
   "source": [
    "# What if we use the kNN classifier from scikit-learn? What will the accuracy be?\n",
    "knnClf = KNeighborsClassifier(n_neighbors=k_for_all)\n",
    "\n",
    "# First train the classifier (fit)\n",
    "knnClf.fit(X_train, y_train)\n",
    "\n",
    "# Then perform prediction\n",
    "y_pred_scl = knnClf.predict(X_test)\n",
    "\n",
    "# And finally get the accuracy of the prediction\n",
    "acc_scl = accuracy_score(y_test, y_pred_scl)\n",
    "\n",
    "# Print it\n",
    "print(f\"Accuracy of scikit-learn's kNN classifier: {acc_scl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78566fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just for curiosity, what's the accuracy between our classifier predictions\n",
    "# and the ones from scikit-learn's?\n",
    "accuracy_score(y_pred_ours, y_pred_scl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe920d6",
   "metadata": {},
   "source": [
    "Perfect accuracy, which means the two classifiers yield the same result! We've\n",
    "just reimplemented a $k$-Nearest Neighbor classifier by \"hand\".\n",
    "\n",
    "Note, however, that `scikit-learn`'s implementation is much more performant, and\n",
    "has several additional options, such as configurable distance metric or giving\n",
    "more weight to closer neighbors (which can be quite important, as we'll discuss\n",
    "in our lecture)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604dda41",
   "metadata": {},
   "source": [
    "### 2.2. A $k$-Nearest Neighbors regressor\n",
    "\n",
    "Now, for the regressor. We'll use the regressor to predict the \"Total\" column\n",
    "in the Pokémon dataset.\n",
    "\n",
    "We'll also compare our results with the respective regressor in `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "856b0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our regressor will also be a simple function that does the training (fit) and\n",
    "# regression (predict) in one go\n",
    "def knn_regress(X_train, y_train, X_test, k=5):\n",
    "\n",
    "    # We similarly obtain the Euclidean distances and the indices of the k\n",
    "    # neighbors closest to each test example\n",
    "    dists = euclidean_distances(X_test, X_train)\n",
    "    idx_mink = np.argpartition(dists, k, axis=1)[:, :k]\n",
    "\n",
    "    # Now simply return the mean of the k closest neighbors\n",
    "    return y_train[idx_mink].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c017d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we want to predict the total, our label data (y) is the total, so let's\n",
    "# get it (note that we don't need to scale the target label)\n",
    "y_total = df[\"Total\"].to_numpy()\n",
    "\n",
    "# And now resplit the X and y into training and test data\n",
    "# We need to do this again since the y labels are different (now it's the\n",
    "# \"Total\", before it was the \"Legendary\" status)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_total, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3029ac00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.832500000000005"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's apply our regressor and obtain the predicted totals\n",
    "y_regr_ours = knn_regress(X_train, y_train, X_test, k=k_for_all)\n",
    "\n",
    "# What's the mean absolute error of our predictions?\n",
    "mean_absolute_error(y_test, y_regr_ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f61774fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031841346483808264"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since variables and labels are scaled, it's difficult to assess how \"small\" or\n",
    "# \"large\" that error was. Therefore, we can use the percentage variant of the\n",
    "# mean absolute error:\n",
    "mean_absolute_percentage_error(y_test, y_regr_ours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5230eefd",
   "metadata": {},
   "source": [
    "An absolute percentage error of 3%. Not bad. Let's check out how the $k$-NN\n",
    "regressor fares in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d594b5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.832500000000005"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second, try out scikit-learn's kNN regressor\n",
    "knnRegr = KNeighborsRegressor(n_neighbors=k_for_all)\n",
    "\n",
    "# Train it (fit)\n",
    "knnRegr.fit(X_train, y_train)\n",
    "\n",
    "# Get prediction (perform regression)\n",
    "y_regr_scl = knnRegr.predict(X_test)\n",
    "\n",
    "# And finally, what's the mean absolute error of this regressor?\n",
    "mean_absolute_error(y_test, y_regr_scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac87bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031841346483808264"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at that error in terms of percentage\n",
    "mean_absolute_percentage_error(y_test, y_regr_scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc5dd14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just for curiosity, what's the difference between our regressor's predictions\n",
    "# and the ones from scikit-learn's?\n",
    "mean_absolute_error(y_regr_ours, y_regr_scl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5922932",
   "metadata": {},
   "source": [
    "No difference between our regressor and `scikit-learn`'s own, which confirms\n",
    "that our code is working."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
